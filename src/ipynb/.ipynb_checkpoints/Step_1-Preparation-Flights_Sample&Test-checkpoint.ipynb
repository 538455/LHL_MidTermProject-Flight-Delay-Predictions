{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd89ca15-6b96-491f-b08e-7a7cb850967d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Function Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fd4687-871c-4a5c-aa42-7e84b8ef7544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataCleaning(df, code=True, tips=False, orientation=True, formatIssues=True, missingValues=True, duplicateValues=True, outliers=True):\n",
    "    \"\"\"\n",
    "    df: your dataframe\n",
    "\n",
    "    code: A text template to note your observations as you go. Use the code snippets included in the output. copy-paste into vscode/notepad\n",
    "\n",
    "    tips: Provides snippets of code to help you clean potential issues in your df. If you prefer this to code\n",
    "    \n",
    "    orientation: Provides information about the shape/objects of your data\n",
    "    \n",
    "    formatIssues: Provides detailed information on each column to help identify format issues\n",
    "    \n",
    "    missingValues: Provides information on missing values\n",
    "    \n",
    "    duplicateValues: Provides information on duplicate values\n",
    "    \n",
    "    outliers: Provides information on outliers\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    if code==True:\n",
    "        print(\"### CLEANING CODE:\")\n",
    "        print(\"df = dfX #Change to your df's name\")\n",
    "        print()\n",
    "        print(\"#### Change column value:\")\n",
    "        print()\n",
    "        print()\n",
    "        print(\"#### Drop entire column:\")\n",
    "        print()\n",
    "        print()\n",
    "        print(\"#### Change column type:\")\n",
    "        print()\n",
    "        print()\n",
    "        print(\"#### Change column name:\")\n",
    "        print()\n",
    "        print()\n",
    "        print(\"#### Handle missing values:\")\n",
    "        print()\n",
    "        print()\n",
    "        print(\"#### Handle duplicate values:\")\n",
    "        print(\"# df.drop_duplicates(inplace=True) # drop ALL duplicate rows\")\n",
    "        print()\n",
    "        print(\"#### Drop outliers:\")\n",
    "        print()\n",
    "        print()\n",
    "        print(\"#### Other observations / further investigations:\")\n",
    "        print(\"#\")\n",
    "        print(\"#\")\n",
    "        print(\"#\")\n",
    "        print()\n",
    "        print(\"df.head() #Final Review\")\n",
    "        print(\"# dfX = df #Change to your df's name\")\n",
    "        print()\n",
    "        print(\"=========================================\")\n",
    "    \n",
    "    if orientation==True:\n",
    "        print(\"ORIENTATION\")\n",
    "        print(df.info())\n",
    "        print(\"=========================================\")\n",
    "        print()\n",
    "        \n",
    "    \n",
    "    if formatIssues==True:\n",
    "        print(\"FORMAT ISSUES\")\n",
    "        print()\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object' or df[col].dtype == 'int64' or df[col].dtype == 'float64' or df[col].dtype == 'datetime64':\n",
    "            #if df[col].dtype == 'float64':\n",
    "\n",
    "                print(\"df.rename(columns={'\" + col + \"': ''}, inplace=True)\", \"#rename column\")\n",
    "                print(\"df['\" + col + \"'] = df['\" + col + \"'].replace('old_value', 'new_value')\")\n",
    "                print(\"df['\" + col + \"'] = df['\" + col + \"'].astype('new_type') # new_type can be int64, float64, object, category, datetime64\")\n",
    "                print(\"df.drop('\" + col + \"', axis=1, inplace=True)\")                \n",
    "                pd.set_option('display.max_rows', None)\n",
    "                print(df.groupby(col, sort=True).size())\n",
    "                pd.reset_option('display.max_rows')\n",
    "                #display the dtypes of the column\n",
    "                print(\"Current Column DType: \", df[col].dtype, \"     Do not compare with above. This one will always return int64 as it's the dtype of the count\")                \n",
    "                print(\"df['\" + col + \"'] = df['\" + col + \"'].astype('new_type') # new_type can be int64, float64, object, category, datetime64\")\n",
    "                print()\n",
    "            #else:\n",
    "            #    print(col)\n",
    "            #    print(df[col].describe())\n",
    "            #    print()\n",
    "\n",
    "        if tips==True:\n",
    "            print(\"TIPS\")\n",
    "            print(\"To make a correction to a column, use the following syntax:\")\n",
    "            print(\"df['A'] = df['A'].apply(lambda x: x.replace('old_value', 'new_value'))\")\n",
    "            print()\n",
    "            print(\"To change a column data type, use the following syntax:\")\n",
    "            print(\"df['A'] = pd.to_datetime(df['A']) # for datetime\")\n",
    "            print(\"df['A'] = df['A'].astype('int64') # for integers\")\n",
    "            print(\"df['A'] = df['A'].astype('float64') # for floats\")\n",
    "            print(\"df['A'] = df['A'].astype('category') # for categorical\")\n",
    "            print(\"df['A'] = df['A'].astype('object') # for object\")\n",
    "            print()\n",
    "        print(\"=========================================\")\n",
    "        print()\n",
    "\n",
    "    if missingValues==True:\n",
    "        print(\"MISSING VALUES\")\n",
    "        print()\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                print(col, \":\", df[col].isnull().sum(), \" missing values\")\n",
    "                print(\"df.dropna(subset=['\" + col + \"'], inplace=True)\")\n",
    "                print(\"df['\" + col + \"'].fillna(df['\" + col + \"'].mean(), inplace=True) #fill NA entries with the mean\")\n",
    "                print(\"df['\" + col + \"'].fillna(0, inplace=True) # fill NA entries with a single value, such as zero\")\n",
    "                print()\n",
    "                print(df.loc[df[col].isnull()].head())\n",
    "                print()\n",
    "            else:\n",
    "                print(col, \": No missing values\")\n",
    "                print()\n",
    "                                    \n",
    "        if tips==True:\n",
    "            print()\n",
    "            print(\"TIPS\")\n",
    "            print(\"You can drop rows with missing values using one of the following code:\")\n",
    "            print(\"df.dropna(subset=['col'], inplace=True) #For a single column\")\n",
    "            print(\"df.dropna(inplace=True) #For all columns\")\n",
    "            print()\n",
    "            print(\"You can fill rows with missing values using one of the following code:\")\n",
    "            print(\"df['col'].fillna(df['col'].mean(), inplace=True) #fill NA entries with the mean\")\n",
    "            print(\"df['col'].fillna(0, inplace=True) # fill NA entries with a single value, such as zero\")\n",
    "            print(\"df['col'].fillna(method='ffill') # forward-fill to propagate the previous value forward\")\n",
    "            print(\"df['col'].fillna(method='bfill' # back-fill to propagate the next values backward)\")\n",
    "            print()\n",
    "            print(\"To view them:\")\n",
    "            print(\"df.loc[df[col].isnull()].head()\")\n",
    "            print()\n",
    "        print(\"=========================================\")\n",
    "        print()\n",
    "\n",
    "    if duplicateValues==True:\n",
    "        print(\"DUPLICATE VALUES\")\n",
    "        print()\n",
    "        print(df[df.duplicated()].head())\n",
    "        print()\n",
    "\n",
    "        if tips==True:\n",
    "            print(\"TIPS\")\n",
    "            print(\"You can drop duplicate rows using the following code:\")\n",
    "            print(\"df.drop_duplicates(inplace=True)\")\n",
    "            print(\"df.drop_duplicates(subset=['col'], inplace=True) #For a single column\")\n",
    "            print()\n",
    "            print(\"To view them:\")\n",
    "            print(\"df[df.duplicated()].head()\")\n",
    "            print()\n",
    "    \n",
    "        print(\"=========================================\")\n",
    "        print()\n",
    "\n",
    "    if outliers==True:\n",
    "        print(\"OUTLIERS\")\n",
    "        print()\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'int64' or df[col].dtype == 'float64':\n",
    "                print(col)\n",
    "                print(\"-----\")\n",
    "                print(\"Outlier(s):\")\n",
    "                print(\"Below \", df[col].mean() - 3*df[col].std(), \" -> \", df[df[col] < df[col].mean() - 3*df[col].std()].shape[0], \" low outlier(s)\")\n",
    "                print(\"Above \", df[col].mean() + 3*df[col].std(), \" -> \", df[df[col] > df[col].mean() + 3*df[col].std()].shape[0], \" high outlier(s)\")\n",
    "                low = df[col].mean() - 3*df[col].std()\n",
    "                high = df[col].mean() + 3*df[col].std()\n",
    "                print(\"df = df[(df['\" + col + \"'] > \" + str(low) + \") & (df['\" + col + \"'] < \" + str(high) + \")]\")\n",
    "                print()\n",
    "                print(df[col].describe())\n",
    "                print()\n",
    "                print(\"Boxplot\")\n",
    "                sns.boxplot(df[col])\n",
    "                plt.show()\n",
    "                print()\n",
    "                print(\"Histogram\")\n",
    "                sns.histplot(df[col])\n",
    "                plt.show()\n",
    "                print(\"=========================================\")\n",
    "                print()\n",
    "\n",
    "        if tips==True:\n",
    "            print(\"TIPS\")\n",
    "            print(\"You can drop outliers using the following code:\")\n",
    "            print(\"df = df[(df['column'] > lower_bound) & (df['column'] < upper_bound)]\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48072217-c0ed-4657-9610-c3cca8d87556",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cd2116-e279-405d-a4e9-b48c6dabb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964e2e5e-18a9-4695-9682-22c0f9e331e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#flights_sample = pd.read_csv(\"../../data/raw/100K_pull_flights(26Nov).csv\")  #This was our first sample which was not pulled randomly. Kept for archive purposes\n",
    "\n",
    "flights_sample = pd.read_csv(\"../../data/raw/200K_random_flights(26Nov).csv\")\n",
    "\n",
    "flights_test = pd.read_csv(\"../../data/raw/FLIGHTS_TEST_RAW.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2fd1c4-e8a5-4c56-b0b8-fcc5fff97d77",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation - Common to Sample & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add9a30-03f9-40dc-8a27-4a0aadd9bd2d",
   "metadata": {},
   "source": [
    "In this step we'll run the function below to identify formatting issues, columns to drop, outliers, missing values, duplicates, etc. In order to keep this notebook more readable, this step will be commented out once completed. The results are consolidated in a cleaning code, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1fb7bc-6a4d-47b2-a7ef-6f9de04592f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataCleaning(flights_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48034235-4f70-4677-b6a8-d4a41e49f733",
   "metadata": {},
   "source": [
    "As we'll be running both the test and sample thru this, we've put it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f791f8db-65d3-45f9-89f6-1655582c8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flights_clean(flights):\n",
    "    ### CLEANING CODE:\n",
    "    df = flights #Change to your df's name\n",
    "\n",
    "    #### Drop columns:\n",
    "    df.drop('branded_code_share', axis=1, inplace=True) # Using Unique Carrier Code for analysis\n",
    "    df.drop('mkt_carrier', axis=1, inplace=True) # Using Unique Carrier Code for analysis\n",
    "    df.drop('mkt_carrier_fl_num', axis=1, inplace=True) #using op_carrier_fl_num instead\n",
    "    df.drop('origin_airport_id', axis=1, inplace=True) #working with IATA codes instead\n",
    "    df.drop('dest_airport_id', axis=1, inplace=True) #working with IATA codes instead\n",
    "    df.drop('dup', axis=1, inplace=True) # All the same value\n",
    "    df.drop('flights', axis=1, inplace=True) # All the same value\n",
    "    df.drop('dest_city_name', axis=1, inplace=True) # Deemed not useful\n",
    "    df.drop('origin_city_name', axis=1, inplace=True) # All the same value\n",
    "\n",
    "    #### Change column type:\n",
    "    df['fl_date'] = df['fl_date'].astype('datetime64')  \n",
    "\n",
    "    #### Change column value:\n",
    "    df['crs_dep_time'] = pd.to_datetime(df['crs_dep_time'], unit='m', errors='coerce').dt.strftime(\"%H:%M\")\n",
    "    df['crs_arr_time'] = pd.to_datetime(df['crs_arr_time'], unit='m', errors='coerce').dt.strftime(\"%H:%M\")\n",
    "\n",
    "    #### Change column name:\n",
    "    df.rename(columns={'fl_date': 'Flight Date'}, inplace=True) \n",
    "    df.rename(columns={'mkt_unique_carrier': 'Marketer - Unique Carrier Code'}, inplace=True)\n",
    "    df.rename(columns={'op_unique_carrier': 'Operator - Unique Carrier Code'}, inplace=True)\n",
    "    df.rename(columns={'op_carrier_fl_num': 'Flight Number'}, inplace=True)\n",
    "    df.rename(columns={'tail_num': 'Tail Number'}, inplace=True) \n",
    "    df.rename(columns={'origin': 'Origin Airport (IATA Code)'}, inplace=True) \n",
    "    df.rename(columns={'dest': 'Destination Airport (IATA Code)'}, inplace=True) \n",
    "    df.rename(columns={'crs_dep_time': 'Scheduled Departure Time (local time)'}, inplace=True) \n",
    "    df.rename(columns={'crs_arr_time': 'Scheduled Arrival Time (local time)'}, inplace=True)\n",
    "    df.rename(columns={'crs_elapsed_time': 'Scheduled Elapsed Time'}, inplace=True)\n",
    "    df.rename(columns={'distance': 'Distance (miles)'}, inplace=True)\n",
    "\n",
    "    # Is op_unique_carrier a duplicate of mkt_unique_carrier? No, we'll keep both and create an add'l column to highlight when they are not the same\n",
    "    df['Different Marketer & Operator Carrier Code'] = np.where(df['Marketer - Unique Carrier Code'] != df['Operator - Unique Carrier Code'], 1, 0)\n",
    "\n",
    "    # Create a column with the day/month/year of the flight\n",
    "    df['Flight Weekday'] = pd.DatetimeIndex(df['Flight Date']).weekday   #0: Monday, 1:Tuesday, etc.\n",
    "    df['Flight Day'] = pd.DatetimeIndex(df['Flight Date']).day\n",
    "    df['Flight Month'] = pd.DatetimeIndex(df['Flight Date']).month\n",
    "    df['Flight Year'] = pd.DatetimeIndex(df['Flight Date']).year\n",
    "    df.drop('Flight Date', axis=1, inplace=True) # Empty column\n",
    "    \n",
    "    # Create hour columns\n",
    "    df[\"Scheduled hour of departure\"] = df[\"Scheduled Departure Time (local time)\"].str.split(\":\").str[0].astype(int)\n",
    "    df[\"Scheduled hour of arrival\"] = df[\"Scheduled Arrival Time (local time)\"].str.split(\":\").str[0].astype(int)\n",
    "\n",
    "    #df.head(10) #Final Review\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4158b23a-7047-4d14-add6-c5621dff4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean both the sample and test dataset\n",
    "flights_sample = flights_clean(flights_sample)\n",
    "flights_test = flights_clean(flights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a920652-b2bf-4292-83b9-be499115b87e",
   "metadata": {},
   "source": [
    "# Export Flights Test to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d801adcf-db74-432a-b670-7d331e4cc37f",
   "metadata": {},
   "source": [
    "This is the end of the road for our flights test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b646540-cf5f-44e9-b539-025d9ddda941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660556, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fb8643-00b0-4b34-ab3d-1d0b5e602dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_test.to_csv('../../data/raw/Cleaned-flights_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36312087-36c8-4970-aa54-62f5d3f71143",
   "metadata": {},
   "source": [
    "We have some additional work to do with the sample one, detailed below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a70ac0-4ec5-4039-89f7-c83f6c541fbf",
   "metadata": {},
   "source": [
    "# Additional cleaning steps for sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f1fb8-5b57-4706-be63-0c3a102b6085",
   "metadata": {},
   "source": [
    "The sample has additional columns, although useful informatino, it is not included in the test so we'll remove most of them.\n",
    "\n",
    "As well, we have the delay information which we'll use at a later step. We'll clean those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff108561-b1db-4968-9031-5dc9bc820055",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEANING CODE (Sample only):\n",
    "df = flights_sample #\n",
    "\n",
    "#### Drop columns\n",
    "df.drop('dep_time', axis=1, inplace=True)\n",
    "df.drop('taxi_out', axis=1, inplace=True)\n",
    "df.drop('taxi_in', axis=1, inplace=True)\n",
    "df.drop('wheels_off', axis=1, inplace=True)\n",
    "df.drop('wheels_on', axis=1, inplace=True)\n",
    "df.drop('arr_time', axis=1, inplace=True)\n",
    "df.drop('actual_elapsed_time', axis=1, inplace=True)\n",
    "df.drop('air_time', axis=1, inplace=True)\n",
    "df.drop('first_dep_time', axis=1, inplace=True) #99125  missing values\n",
    "df.drop('total_add_gtime', axis=1, inplace=True) #99125  missing values\n",
    "df.drop('longest_add_gtime', axis=1, inplace=True) #99125  missing values\n",
    "df.drop('no_name', axis=1, inplace=True)\n",
    "\n",
    "### Formatting the additional delay columns:\n",
    "# Departure Delay\n",
    "df['dep_delay'] = df['dep_delay'].fillna(0)    #Didn't really have an issue with this one, but just in case\n",
    "df['dep_delay'] = df['dep_delay'].astype('int64')\n",
    "df.rename(columns={'dep_delay': 'Departure Delay (minutes)'}, inplace=True)\n",
    "\n",
    "# Arrival Delay\n",
    "df['arr_delay'] = df['arr_delay'].fillna(0)\n",
    "df['arr_delay'] = df['arr_delay'].astype('int64')\n",
    "df.rename(columns={'arr_delay': 'Arrival Delay (minutes)'}, inplace=True)\n",
    "\n",
    "# carrier_delay\n",
    "df['carrier_delay'] = df['carrier_delay'].fillna(0)\n",
    "df['carrier_delay'] = df['carrier_delay'].astype('int64')\n",
    "df.rename(columns={'carrier_delay': 'Carrier Delay (minutes)'}, inplace=True)\n",
    "\n",
    "# weather_delay\n",
    "df['weather_delay'] = df['weather_delay'].fillna(0)\n",
    "df['weather_delay'] = df['weather_delay'].astype('int64')\n",
    "df.rename(columns={'weather_delay': 'Weather Delay (minutes)'}, inplace=True)\n",
    "\n",
    "# nas_delay\n",
    "df['nas_delay'] = df['nas_delay'].fillna(0)\n",
    "df['nas_delay'] = df['nas_delay'].astype('int64')\n",
    "df.rename(columns={'nas_delay': 'National Air System Delay (minutes)'}, inplace=True)\n",
    "\n",
    "# security_delay\n",
    "df['security_delay'] = df['security_delay'].fillna(0)\n",
    "df['security_delay'] = df['security_delay'].astype('int64')\n",
    "df.rename(columns={'security_delay': 'Security Delay (minutes)'}, inplace=True)\n",
    "\n",
    "# late_aircraft_delay\n",
    "df['late_aircraft_delay'] = df['late_aircraft_delay'].fillna(0)\n",
    "df['late_aircraft_delay'] = df['late_aircraft_delay'].astype('int64')\n",
    "df.rename(columns={'late_aircraft_delay': 'Late Aircraft Delay (minutes)'}, inplace=True)\n",
    "\n",
    "#df.head(10) #Final Review\n",
    "flights_sample = df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8d064-0216-43b0-bb30-fbd3afc7cf11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Below (collapsed) is the first Cleaning Code we used... Only kept for archive purposes..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce6b7f7a-4589-4df8-a514-ad8aa8f639e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CLEANING CODE:\n",
    "df = flights_sample #Change to your df's name\n",
    "\n",
    "#### Handle missing values:\n",
    "# Missing values, but GTG (keep):\n",
    "# cancellation_code : 97030  missing values\n",
    "##df.drop('taxi_out', axis=1, inplace=True) #3075  missing values -> KEEP\n",
    "##df.drop('wheels_off', axis=1, inplace=True) #3075  missing values -> KEEP\n",
    "##df.drop('wheels_on', axis=1, inplace=True) #3130  missing values -> KEEP\n",
    "##df.drop('taxi_in', axis=1, inplace=True) #3130  missing values -> KEEP\n",
    "\n",
    "\n",
    "#Fill missing values with 0:\n",
    "#df['dep_time'] = df['dep_time'].fillna(0)    ### WATCHOUT FOR MIDNIGHTS!! if keeping them\n",
    "#df['arr_time'] = df['arr_time'].fillna(0)    ### WATCHOUT FOR MIDNIGHTS!! if keeping them \n",
    "\n",
    "df['dep_delay'] = df['dep_delay'].fillna(0)    #Didn't really have an issue with this one, but just in case\n",
    "df['arr_delay'] = df['arr_delay'].fillna(0)\n",
    "df['actual_elapsed_time'] = df['actual_elapsed_time'].fillna(0)\n",
    "df['air_time'] = df['air_time'].fillna(0)\n",
    "df['carrier_delay'] = df['carrier_delay'].fillna(0)\n",
    "df['weather_delay'] = df['weather_delay'].fillna(0)\n",
    "df['nas_delay'] = df['nas_delay'].fillna(0)\n",
    "df['security_delay'] = df['security_delay'].fillna(0)\n",
    "df['late_aircraft_delay'] = df['late_aircraft_delay'].fillna(0)\n",
    "\n",
    "# Drop rows:\n",
    "df.dropna(subset=['tail_num'], inplace=True) # Drop rows with missing values in Tail Number? # 580 tail numbers missing\n",
    "\n",
    "# Drop columns:\n",
    "df.drop('first_dep_time', axis=1, inplace=True) #99125  missing values\n",
    "df.drop('total_add_gtime', axis=1, inplace=True) #99125  missing values\n",
    "df.drop('longest_add_gtime', axis=1, inplace=True) #99125  missing values\n",
    "\n",
    "# Do we even need these 4x columns? or do we calculate to fill (dropping for now):\n",
    "# taxi_out : 3075  missing values       # Calculate time between wheels_off and dep_time?\n",
    "# wheels_off : 3075  missing values         # Wheels on minus air time?\n",
    "# wheels_on : 3130  missing values          # Wheels off plus air time?\n",
    "# taxi in : 3130  missing values            # Calculate time between wheels_on and arr_time?\n",
    "\n",
    "#### Change column type:\n",
    "df['fl_date'] = df['fl_date'].astype('datetime64')  \n",
    "df['dep_delay'] = df['dep_delay'].astype('int64')\n",
    "df['arr_delay'] = df['arr_delay'].astype('int64')\n",
    "df['actual_elapsed_time'] = df['actual_elapsed_time'].astype('int64')\n",
    "df['air_time'] = df['air_time'].astype('int64')\n",
    "df['carrier_delay'] = df['carrier_delay'].astype('int64')\n",
    "df['weather_delay'] = df['weather_delay'].astype('int64')\n",
    "df['nas_delay'] = df['nas_delay'].astype('int64')\n",
    "df['security_delay'] = df['security_delay'].astype('int64')\n",
    "df['late_aircraft_delay'] = df['late_aircraft_delay'].astype('int64')\n",
    "\n",
    "#If not dropped:\n",
    "#df['first_dep_time'] = df['first_dep_time'].astype('int64')\n",
    "#df['total_add_gtime'] = df['total_add_gtime'].astype('int64')\n",
    "#df['longest_add_gtime'] = df['longest_add_gtime'].astype('int64')\n",
    "\n",
    "\n",
    "#### Change column value:\n",
    "df['crs_dep_time'] = pd.to_datetime(df['crs_dep_time'], unit='m', errors='coerce').dt.strftime(\"%H:%M\")\n",
    "df['crs_arr_time'] = pd.to_datetime(df['crs_arr_time'], unit='m', errors='coerce').dt.strftime(\"%H:%M\")\n",
    "df['dep_time'] = pd.to_datetime(df['dep_time'], unit='m', errors='coerce').dt.strftime(\"%H:%M\")\n",
    "df['arr_time'] = pd.to_datetime(df['arr_time'], unit='m', errors='coerce').dt.strftime(\"%H:%M\")\n",
    "df['wheels_off'] = pd.to_datetime(df['wheels_off'], unit='m', errors='coerce').dt.strftime(\"%H:%M\")\n",
    "df['wheels_on'] = pd.to_datetime(df['wheels_on'], unit='m', errors='coerce').dt.strftime(\"%H:%M\")\n",
    "df['taxi_out'] = pd.to_datetime(df['taxi_out'], unit='m', errors='coerce').dt.strftime(\"%H:%M\")\n",
    "df['taxi_in'] = pd.to_datetime(df['taxi_in'], unit='m', errors='coerce').dt.strftime(\"%H:%M\")\n",
    "\n",
    "#If not dropped:\n",
    "#df['first_dep_time'] = pd.to_datetime(df['first_dep_time'], unit='m', errors='coerce').dt.strftime(\"%H:%M\")\n",
    "\n",
    "\n",
    "#### Drop entire column:\n",
    "df.drop('branded_code_share', axis=1, inplace=True) # drop if not useful as Unique Carrier Code is the recommended one\n",
    "df.drop('mkt_carrier', axis=1, inplace=True) # drop if not useful as Unique Carrier Code is the recommended one\n",
    "df.drop('dup', axis=1, inplace=True) # All the same value\n",
    "df.drop('flights', axis=1, inplace=True) # All the same value\n",
    "df.drop('no_name', axis=1, inplace=True) # Empty column\n",
    "\n",
    "#### Handle duplicate values:\n",
    "#None. \n",
    "\n",
    "#### Drop outliers:\n",
    "\n",
    "#GTG:\n",
    "# op_carrier_fl_num\n",
    "\n",
    "#Maybe drop:\n",
    "# df = df[(df['dep_delay'] > -134.73225994298653) & (df['dep_delay'] < 157.16590376279444)]\n",
    "# df = df[(df['taxi_out'] > -17.489648919033392) & (df['taxi_out'] < 54.755926969072085)]\n",
    "# df = df[(df['taxi_in'] > -10.45644040640464) & (df['taxi_in'] < 25.276756293676243)]\n",
    "\n",
    "\n",
    "\n",
    "#### Change column name:\n",
    "df.rename(columns={'fl_date': 'Flight Date'}, inplace=True) \n",
    "df.rename(columns={'mkt_unique_carrier': 'Marketer - Unique Carrier Code'}, inplace=True)\n",
    "df.rename(columns={'op_unique_carrier': 'Operator - Unique Carrier Code'}, inplace=True)\n",
    "df.rename(columns={'op_carrier_fl_num': 'Flight Number'}, inplace=True)\n",
    "df.rename(columns={'tail_num': 'Tail Number'}, inplace=True) \n",
    "df.rename(columns={'origin_airport_id': 'Origin Airport (ID)'}, inplace=True) \n",
    "df.rename(columns={'origin': 'Origin Airport (IATA Code)'}, inplace=True) \n",
    "df.rename(columns={'origin_city_name': 'Origin Airport (City, State)'}, inplace=True) \n",
    "df.rename(columns={'dest_airport_id': 'Destination Airport (ID)'}, inplace=True) \n",
    "df.rename(columns={'dest': 'Destination Airport (IATA Code)'}, inplace=True) \n",
    "df.rename(columns={'dest_city_name': 'Destination Airport (City, State)'}, inplace=True) \n",
    "df.rename(columns={'crs_dep_time': 'Scheduled Departure Time (local time)'}, inplace=True) \n",
    "df.rename(columns={'dep_time': 'Actual Departure Time (local time)'}, inplace=True)\n",
    "df.rename(columns={'dep_delay': 'Departure Delay (minutes)'}, inplace=True)\n",
    "df.rename(columns={'crs_arr_time': 'Scheduled Arrival Time (local time)'}, inplace=True)\n",
    "df.rename(columns={'arr_time': 'Actual Arrival Time (local time)'}, inplace=True)\n",
    "df.rename(columns={'arr_delay': 'Arrival Delay (minutes)'}, inplace=True)\n",
    "df.rename(columns={'cancelled': 'Cancelled'}, inplace=True)\n",
    "df.rename(columns={'cancellation_code': 'Cancellation Code'}, inplace=True)\n",
    "df.rename(columns={'diverted': 'Diverted'}, inplace=True)\n",
    "df.rename(columns={'crs_elapsed_time': 'Scheduled Elapsed Time'}, inplace=True)\n",
    "df.rename(columns={'actual_elapsed_time': 'Actual Elapsed Time'}, inplace=True)\n",
    "df.rename(columns={'air_time': 'Air Time'}, inplace=True)\n",
    "df.rename(columns={'distance': 'Distance (miles)'}, inplace=True)\n",
    "df.rename(columns={'carrier_delay': 'Carrier Delay (minutes)'}, inplace=True)\n",
    "df.rename(columns={'weather_delay': 'Weather Delay (minutes)'}, inplace=True)\n",
    "df.rename(columns={'nas_delay': 'National Air System Delay (minutes)'}, inplace=True)\n",
    "df.rename(columns={'security_delay': 'Security Delay (minutes)'}, inplace=True)\n",
    "df.rename(columns={'late_aircraft_delay': 'Late Aircraft Delay (minutes)'}, inplace=True)\n",
    "df.rename(columns={'taxi_out': 'Taxi Out (minutes)'}, inplace=True) \n",
    "df.rename(columns={'wheels_off': 'Wheels Off (local time)'}, inplace=True) \n",
    "df.rename(columns={'wheels_on': 'Wheels On (local time)'}, inplace=True)   \n",
    "df.rename(columns={'taxi_in': 'Taxi In (minutes)'}, inplace=True)\n",
    "\n",
    "#If not dropped:\n",
    "# df.rename(columns={'mkt_carrier_fl_num': 'Flight Number'}, inplace=True) # We decided to keep only op_carrier_fl_num\n",
    "# df.rename(columns={'dup': 'Duplicate'}, inplace=True) # Rename if not dropped\n",
    "# df.rename(columns={'flights': 'Number of Flights'}, inplace=True) #Rename if not dropped\n",
    "# df.rename(columns={'branded_code_share': 'Branded Code Share Partners'}, inplace=True) # Rename if not dropped\n",
    "# df.rename(columns={'mkt_carrier': 'IATA Carrier Code'}, inplace=True) # Rename if not dropped\n",
    "# df.rename(columns={'first_dep_time': 'First Gate Departure Time at Origin Airport'}, inplace=True) #Rename if not dropped\n",
    "# df.rename(columns={'total_add_gtime': 'Total Ground Time Away from Gate for Gate Return or Cancelled Flight'}, inplace=True) #Rename if not dropped\n",
    "# df.rename(columns={'longest_add_gtime': 'Longest Time Away from Gate for Gate Return or Cancelled Flight'}, inplace=True) #Rename if not dropped\n",
    "\n",
    "\n",
    "#### Other observations / further investigations:\n",
    "# Is op_carrier_fl_num a duplicate of mkt_carrier_fl_num? No, but they are similar enough that we'll keep only op_carrier_fl_num\n",
    "# Are there any values where dup is not 'N'? No, all values in the SQL DB are N\n",
    "# Are there any values where flight is > 1? No, all values in the SQL DB are = 1\n",
    "# Are codeshares creating duplicate rows? No\n",
    "\n",
    "# Is op_unique_carrier a duplicate of mkt_unique_carrier? No, we'll keep both and create an add'l column to highlight when they are not the same\n",
    "df['Different Marketer & Operator Carrier Code'] = np.where(df['Marketer - Unique Carrier Code'] != df['Operator - Unique Carrier Code'], 1, 0)\n",
    "\n",
    "# Why do some flights have NaN/Os scheduled/actual departure/arrival time?\n",
    "\n",
    "# Create a column with the day/month/year of the flight\n",
    "df['Flight Weekday'] = pd.DatetimeIndex(df['Flight Date']).weekday   #0: Monday, 1:Tuesday, etc.\n",
    "df['Flight Day'] = pd.DatetimeIndex(df['Flight Date']).day\n",
    "df['Flight Month'] = pd.DatetimeIndex(df['Flight Date']).month\n",
    "df['Flight Year'] = pd.DatetimeIndex(df['Flight Date']).year\n",
    "df.drop('Flight Date', axis=1, inplace=True) # Empty column\n",
    "\n",
    "# To make things pretty, let's re-order the columns\n",
    "df = df.reindex(columns=['Flight Year', 'Flight Month','Flight Day', 'Flight Weekday', 'Marketer - Unique Carrier Code', 'Operator - Unique Carrier Code', 'Different Marketer & Operator Carrier Code', 'Tail Number', 'Flight Number',  'Origin Airport (ID)', 'Origin Airport (IATA Code)', 'Origin Airport (City, State)', 'Destination Airport (ID)', 'Destination Airport (IATA Code)', 'Destination Airport (City, State)', 'Departure Delay (minutes)',  'Arrival Delay (minutes)', 'Cancelled', 'Cancellation Code', 'Diverted', 'Scheduled Departure Time (local time)', 'Actual Departure Time (local time)', 'Taxi Out (minutes)', 'Wheels Off (local time)', 'Wheels On (local time)', 'Taxi In (minutes)', 'Scheduled Arrival Time (local time)', 'Actual Arrival Time (local time)', 'Scheduled Elapsed Time', 'Actual Elapsed Time', 'Air Time', 'Distance (miles)', 'Carrier Delay (minutes)', 'Weather Delay (minutes)', 'National Air System Delay (minutes)', 'Security Delay (minutes)', 'Late Aircraft Delay (minutes)'])\n",
    "\n",
    "#df.head(10) #Final Review\n",
    "flights_sample = df #Change to your df's name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7506195-5a1a-4901-ba12-bc2816e293c0",
   "metadata": {},
   "source": [
    "# Split the sample into a Train and a Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05eb39bd-edb8-4d16-b4b4-9995c461a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c4a1882-0779-4fc2-9e34-8530a3a70fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_sample_train, flights_sample_test = train_test_split(flights_sample, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e378440-2313-4f94-8c0f-b4824be4534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_sample_train = flights_sample_train.reset_index().drop('index', axis=1)\n",
    "#flights_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f655c1-fc9f-4986-98b3-1feb01963368",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_sample_test = flights_sample_test.reset_index().drop('index', axis=1)\n",
    "#flights_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c07fba-998e-44f7-a0f3-a62e9cdc6a5f",
   "metadata": {},
   "source": [
    "# Additional Steps for the training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038abb7-8c61-499c-90f9-51bb23e89310",
   "metadata": {},
   "source": [
    "We decided to remove the outliers from the training set to try to increase accuracy. We've also removed the cancelled flights and the diverted flights in the same efforts. As this is not something we can do for the test, we are only applying this step to the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "573a0782-1788-4f58-aa75-9cab3f7b5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEANING CODE (Trg only):\n",
    "df = flights_sample_train #\n",
    "\n",
    "# Remove cancelled flights\n",
    "df = df[df['cancelled'] == 0]\n",
    "\n",
    "# Remove diverted flights\n",
    "df = df[df['diverted'] == 0]\n",
    "\n",
    "# Removing 'Departure Delay (minutes)'s outliers\n",
    "low = df['Departure Delay (minutes)'].mean() - (3 * df['Departure Delay (minutes)'].std())\n",
    "high = df['Departure Delay (minutes)'].mean() + (3 * df['Departure Delay (minutes)'].std())\n",
    "df = df[(df['Departure Delay (minutes)'] > low) & (df['Departure Delay (minutes)'] < high)]\n",
    "\n",
    "# Removing 'Arrival Delay (minutes)'s outliers\n",
    "low = df['Arrival Delay (minutes)'].mean() - (3 * df['Arrival Delay (minutes)'].std())\n",
    "high = df['Arrival Delay (minutes)'].mean() + (3 * df['Arrival Delay (minutes)'].std())\n",
    "df = df[(df['Arrival Delay (minutes)'] > low) & (df['Arrival Delay (minutes)'] < high)]\n",
    "\n",
    "flights_sample_train = df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301f8c7-92b6-401d-9c92-74f296aeffe2",
   "metadata": {},
   "source": [
    "# Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c430934-5f1e-4088-8233-9f572c06e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_sample_train.to_csv('../../data/raw/Cleaned-flights_sample_training.csv', index=False)\n",
    "flights_sample_test.to_csv('../../data/raw/Cleaned-flights_sample_testing.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf635d-a32c-497e-bdee-b13e5589fb44",
   "metadata": {},
   "source": [
    "We'll also store a flights sample+test, but only for testing purposes for the enrichments (ie. making sure we have all the airports and such). This will not be used in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8293ee84-f3e6-4876-92f9-dc989c6c591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = pd.concat([flights_test, flights_sample], axis=0)\n",
    "flights = flights[flights_test.columns]\n",
    "\n",
    "flights.to_csv('../../data/raw/flights_sample+test.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
